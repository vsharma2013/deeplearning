{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishal/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import h5py\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from planar_utils import plot_decision_boundary, load_planar_dataset, load_extra_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dnn_app_utils_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9ffa71d44328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_app_utils_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnn_app_utils_v2'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('./datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('./datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "    X -- dataset\n",
    "    y -- true labels\n",
    "    p -- predictions\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n",
    "        \n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.644633\n",
      "Cost after iteration 200: 0.643983\n",
      "Cost after iteration 300: 0.643974\n",
      "Cost after iteration 400: 0.643974\n",
      "Cost after iteration 500: 0.643974\n",
      "Cost after iteration 600: 0.643974\n",
      "Cost after iteration 700: 0.643974\n",
      "Cost after iteration 800: 0.643974\n",
      "Cost after iteration 900: 0.643974\n",
      "Cost after iteration 1000: 0.643974\n",
      "Cost after iteration 1100: 0.643974\n",
      "Cost after iteration 1200: 0.643974\n",
      "Cost after iteration 1300: 0.643974\n",
      "Cost after iteration 1400: 0.643974\n",
      "Cost after iteration 1500: 0.643974\n",
      "Cost after iteration 1600: 0.643974\n",
      "Cost after iteration 1700: 0.643974\n",
      "Cost after iteration 1800: 0.643974\n",
      "Cost after iteration 1900: 0.643974\n",
      "Cost after iteration 2000: 0.643974\n",
      "Cost after iteration 2100: 0.643974\n",
      "Cost after iteration 2200: 0.643974\n",
      "Cost after iteration 2300: 0.643974\n",
      "Cost after iteration 2400: 0.643974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucHGWd7/HPNzOTzECmhwQGFiEhwQ1ezsqKRpCDKLrCxl0X1l3BZHUB9xzxFvcs3hb3+EIOHs7LIyKrC54VFZBVQETFiFljvIEiaBIkYBIDMYAZw2XIBXJBJjP5nT+qmql0untqQio9mfq+X69+TdfTT1U/lYb+9lNP1VOKCMzMzEYyodUNMDOz/YMDw8zMcnFgmJlZLg4MMzPLxYFhZma5ODDMzCwXB4aViqT/lHROq9thtj9yYNg+IekhSa9vdTsi4g0R8eVWtwNA0k8k/fd98D6TJF0t6SlJj0p6/wj1z0/rPZmuNyktny5pa80jJH0gff0USTtrXnc4jyMODBs3JLW3ug1VY6ktwEXALOAo4LXAhyXNqVdR0p8DFwB/BswAjgb+F0BE/C4iJlcfwEuAncA3MptYn60zVsLZ9g4HhrWcpDdKukfSZkk/l3Rs5rULJP1W0hZJKyW9KfPauZLukHS5pI3ARWnZzyR9StImSQ9KekNmnWd/1eeoO1PS7el7/0DSlZK+0mAfTpHUJ+mfJT0KXCNpiqRbJfWn279V0pFp/UuAk4Er0l/iV6TlL5S0WNJGSaslnbUX/onPBj4eEZsiYhXwBeDcBnXPAb4UESsiYhPw8SZ1zwZuj4iH9kIbbT/gwLCWkvQy4GrgncDBwOeBBdXDIMBvSb5Ye0h+6X5F0uGZTZwArAUOBS7JlK0GDgE+CXxJkho0oVnd64Ffpu26CPj7EXbnj4CpJL/kzyP5/+uadHk68DRwBUBE/E/gp8D89Jf4fEkHAovT9z0UmAd8TtJ/qfdmkj6Xhmy9x71pnSnA84DlmVWXA3W3mZbX1j1M0sF16p4N1PYgDpX0WBq+l6f7ZOOEA8Na7R3A5yPiFxExlB7CeAZ4JUBEfD0i1kfEzoj4GvAAcHxm/fUR8W8RMRgRT6dlD0fEFyJiiOQL7XDgsAbvX7eupOnAK4ALI2IgIn4GLBhhX3YCH4uIZyLi6YjYEBHfiIjtEbGFJNBe02T9NwIPRcQ16f7cTXK45831KkfEeyLioAaPai9tcvr3ycyqTwLdDdowuU5dautLOpnk3/TmTPFvgJeS/Bu+Dng58Okm+2v7GQeGtdpRwAeyv46BaSS/ipF0duZw1WbgT0h6A1Xr6mzz0eqTiNiePp1cp16zus8DNmbKGr1XVn9E/KG6IOkASZ+X9LCkp4DbgYMktTVY/yjghJp/i7eS9Fz21Nb0byVTVgG2NKlfW5c69c8BvhER1e0TEY9GxMo03B8EPkyDsLP9kwPDWm0dcEnNr+MDIuIGSUeRHG+fDxwcEQcBvwayh5eKmm75EWCqpAMyZdNGWKe2LR8AXgCcEBEV4NVpuRrUXwfcVvNvMTki3l3vzST9e52zlqqPFQDpOMQjwJ9mVv1TYEWDfVhRp+5jEbEh875dwJnsfjiqVrDrZ2X7OQeG7Usdkjozj3aSQHiXpBOUOFDSX0rqBg4k+dLpB5D0dpIeRuEi4mFgKclA+kRJJwJ/NcrNdJOMW2yWNBX4WM3rj5GchVR1K3CMpL+X1JE+XiHpRQ3a+K6aM5Kyj+wYxXXAR9NB+BeSHAa8tkGbrwP+m6QXp+MfH61T903AZuDH2cJ04H96+jlOAz4BfLvB+9h+yIFh+9JCki/Q6uOiiFhK8gV2BbAJWEN6Vk5ErAQuA+4k+XJ9CXDHPmzvW4ETgQ3A/wa+RjK+kte/Al3AE8BdwPdqXv8M8Ob0DKrPpuMcpwFzgfUkh8v+LzCJ5+ZjJCcPPAzcBlwaEd+DXa6tmA6Qln+SJAweTh+1QXcOcF3sfjOdl5F8VtuAn5P0Bv/xObbdxhD5Bkpm+Uj6GvCbiKj9AjUrBfcwzBpIDwc9X9IEJRe6nQHc0up2mbXKWLoa1Wys+SPgmyTXYfQB746IX7W2SWat40NSZmaWiw9JmZlZLuPmkNQhhxwSM2bMaHUzzMz2K8uWLXsiInrz1B03gTFjxgyWLl3a6maYme1XJD2ct64PSZmZWS4ODDMzy8WBYWZmuTgwzMwsFweGmZnl4sAwM7NcHBhmZpZL6QNjyx92cPni+7ln3eZWN8XMbEwrfWAM7Qw+88MHuPvhTa1uipnZmFb6wOju7ADgyad3tLglZmZjW+kDo22C6J7UzlN/cGCYmTVT+sAAqHR1uIdhZjYCBwZJYDz19GCrm2FmNqY5MICernaecg/DzKwpBwZQ6ezwGIaZ2QgcGECPxzDMzEbkwKA6huHAMDNrxoFB0sPYNjDEjqGdrW6KmdmY5cAAKp3JnWq3/MFnSpmZNeLAAHoO8NXeZmYjcWCQnCUFeBzDzKwJBwbJGAa4h2Fm1owDg+QsKcDXYpiZNVFoYEiaI2m1pDWSLqjz+uWS7kkf90vanHltuqTvS1olaaWkGUW10z0MM7ORtRe1YUltwJXAqUAfsETSgohYWa0TEedn6r8POC6zieuASyJisaTJQGHnvA6PYfgsKTOzRorsYRwPrImItRExANwInNGk/jzgBgBJLwbaI2IxQERsjYjtRTW0s2MCE9smuIdhZtZEkYFxBLAus9yXlu1G0lHATOBHadExwGZJ35T0K0mXpj2W2vXOk7RU0tL+/v49bqgkKl2+J4aZWTNFBobqlEWDunOBmyNiKF1uB04GPgi8AjgaOHe3jUVcFRGzI2J2b2/vc2qs74lhZtZckYHRB0zLLB8JrG9Qdy7p4ajMur9KD2cNArcALyuklalKp+eTMjNrpsjAWALMkjRT0kSSUFhQW0nSC4ApwJ01606RVO02vA5YWbvu3tTjCQjNzJoqLDDSnsF8YBGwCrgpIlZIuljS6Zmq84AbIyIy6w6RHI76oaT7SA5vfaGotkI6Y63nkjIza6iw02oBImIhsLCm7MKa5YsarLsYOLawxtXo6Wr3GIaZWRO+0jtVHcPIdHTMzCzDgZHq6epgcGewfWBo5MpmZiXkwEhVPD2ImVlTDoxUjycgNDNryoGRqs4n9eR2B4aZWT0OjNRwD8On1pqZ1ePASFW6kjOMPYZhZlafAyP1bA/DgWFmVpcDI9Xd6bOkzMyacWCk2iaI7kme4tzMrBEHRoanODcza8yBkVHp6vBtWs3MGnBgZFQ62z3obWbWgAMjo6erw2MYZmYNODAyPIZhZtaYAyPDd90zM2vMgZFR6exg28AQO4Z2tropZmZjjgMjoyedHmSL55MyM9uNAyPD98QwM2vMgZHh+aTMzBpzYGS4h2Fm1pgDI8N33TMza6zQwJA0R9JqSWskXVDn9csl3ZM+7pe0ueb1iqTfS7qiyHZWVTxjrZlZQ+1FbVhSG3AlcCrQByyRtCAiVlbrRMT5mfrvA46r2czHgduKamOt4TEMnyVlZlaryB7G8cCaiFgbEQPAjcAZTerPA26oLkh6OXAY8P0C27iLzo4JdLTJPQwzszqKDIwjgHWZ5b60bDeSjgJmAj9KlycAlwEfavYGks6TtFTS0v7+/ufcYEmeT8rMrIEiA0N1yqJB3bnAzRExlC6/B1gYEesa1E82FnFVRMyOiNm9vb3PoanDPJ+UmVl9hY1hkPQopmWWjwTWN6g7F3hvZvlE4GRJ7wEmAxMlbY2I3QbO97ZKp+eTMjOrp8jAWALMkjQT+D1JKPxdbSVJLwCmAHdWyyLirZnXzwVm74uwgGTge/P2gX3xVmZm+5XCDklFxCAwH1gErAJuiogVki6WdHqm6jzgxohodLhqn6p0dfCU55IyM9tNkT0MImIhsLCm7MKa5YtG2Ma1wLV7uWkN9XS1ewzDzKwOX+ldozqGMUY6PGZmY4YDo0ZPVweDO4PtA0MjVzYzKxEHRo2K55MyM6vLgVGjxzPWmpnV5cCoUZ2A0PNJmZntyoFRwz0MM7P6HBg1Kul9vX21t5nZrhwYNdzDMDOrz4FRo7vTZ0mZmdXjwKjRNkF0T/LV3mZmtRwYdXiKczOz3Tkw6qh0dfi0WjOzGg6MOiqd7T5LysyshgOjDt+m1cxsdw6MOjyGYWa2OwdGHT1dvk2rmVktB0Ydlc4Otg0MsWNoZ6ubYmY2Zjgw6uhJpwfZ4lu1mpk9y4FRR8XTg5iZ7caBUUd1PimPY5iZDXNg1OEehpnZ7hwYdfT4Nq1mZrspNDAkzZG0WtIaSRfUef1ySfekj/slbU7LXyrpTkkrJN0r6S1FtrNW9a577mGYmQ1rL2rDktqAK4FTgT5giaQFEbGyWicizs/Ufx9wXLq4HTg7Ih6Q9DxgmaRFEbG5qPZmDY9h+CwpM7OqInsYxwNrImJtRAwANwJnNKk/D7gBICLuj4gH0ufrgceB3gLbuovOjgl0tMk9DDOzjCID4whgXWa5Ly3bjaSjgJnAj+q8djwwEfhtndfOk7RU0tL+/v690uh0u55PysysRpGBoTpl0aDuXODmiBjaZQPS4cB/AG+PiN0uu46IqyJidkTM7u3dux2QSqfnkzIzyyoyMPqAaZnlI4H1DerOJT0cVSWpAnwX+GhE3FVIC5uoeD4pM7NdFBkYS4BZkmZKmkgSCgtqK0l6ATAFuDNTNhH4FnBdRHy9wDY25MAwM9tVYYEREYPAfGARsAq4KSJWSLpY0umZqvOAGyMie7jqLODVwLmZ025fWlRb60nGMHyWlJlZVWGn1QJExEJgYU3ZhTXLF9VZ7yvAV4ps20gqne0ewzAzy/CV3g1U74mxa8fHzKy8HBgNVLo6GNwZbB8YGrmymVkJODAa8HxSZma7cmA04PmkzMx25cBowPNJmZntKldgSDozT9l4Uklv0+oehplZIm8P4yM5y8YN33XPzGxXTa/DkPQG4C+AIyR9NvNSBRjXx2o8hmFmtquRLtxbDywFTgeWZcq3AOfXXWOc6O5M/ml8lpSZWaJpYETEcmC5pOsjYgeApCnAtIjYtC8a2CrtbROYPMlXe5uZVeUdw1gsqSJpKrAcuEbSpwts15iQXO09ro+8mZnlljcweiLiKeBvgGsi4uXA64tr1tjQ7fmkzMyelTcw2tObGZ0F3Fpge8YU33XPzGxY3sC4mGSa8t9GxBJJRwMPFNesscH3xDAzG5ZrevP0JkZfzyyvBf62qEaNFT1dHaxwYJiZAfmv9D5S0rckPS7pMUnfkHRk0Y1rNd/X28xsWN5DUteQ3F71ecARwHfSsnGtp6uDbQND7Bja2eqmmJm1XN7A6I2IayJiMH1cC/QW2K4xoSedT2qLb9VqZpY7MJ6Q9DZJbenjbcCGIhs2FlS6PD2ImVlV3sD4B5JTah8FHgHeDLy9qEaNFZ6A0MxsWK6zpICPA+dUpwNJr/j+FEmQjFvuYZiZDcvbwzg2O3dURGwEjiumSWOHb9NqZjYsb2BMSCcdBJ7tYYzYO5E0R9JqSWskXVDn9csl3ZM+7pe0OfPaOZIeSB/n5GznXuUpzs3MhuU9JHUZ8HNJNwNBMp5xSbMVJLUBVwKnAn3AEkkLImJltU5EnJ+p/z7SXksaSB8DZqfvtyxdd5/OkOvbtJqZDcvVw4iI60iu7H4M6Af+JiL+Y4TVjgfWRMTaiBgAbgTOaFJ/HnBD+vzPgcURsTENicXAnDxt3Zs6OybQ0Sb3MMzMyN/DIO0ZrByx4rAjgHWZ5T7ghHoVJR0FzAR+1GTdI+qsdx5wHsD06dNH0bR8JHkCQjOzVN4xjD2hOmXRoO5c4OaIGBrNuhFxVUTMjojZvb3FXEfo6UHMzBJFBkYfMC2zfCTJLV/rmcvw4ajRrlsoz1hrZpYoMjCWALMkzZQ0kSQUFtRWkvQCYApwZ6Z4EXCapCnp2VmnpWX7nAPDzCxRWGBExCAwn+SLfhVwU0SskHSxpNMzVecBN0ZEZNbdSHKx4JL0cXFats8lYxg+S8rMLPeg956IiIXAwpqyC2uWL2qw7tXA1YU1LqeKb9NqZgYUe0hqXOhJD0llOkBmZqXkwBhBpauDwZ3B9oGhkSubmY1jDowReD4pM7OEA2MEnk/KzCzhwBiB55MyM0s4MEZQSW/T6h6GmZWdA2MEvuuemVnCgTECj2GYmSUcGCPo7kwOSfksKTMrOwfGCNrbJjB5kq/2NjNzYOSQXO3ts6TMrNwcGDl0ez4pMzMHRh6+656ZmQMjF98Tw8zMgZFLjwPDzMyBkYfv621m5sDIpaerg20DQwwO7Wx1U8zMWsaBkUN1PinfqtXMysyBkYPnkzIzc2Dk4vmkzMwcGLn0HOC77pmZOTBycA/DzKzgwJA0R9JqSWskXdCgzlmSVkpaIen6TPkn07JVkj4rSUW2tRnfdc/MDNqL2rCkNuBK4FSgD1giaUFErMzUmQV8BDgpIjZJOjQt/6/AScCxadWfAa8BflJUe5vxXffMzIrtYRwPrImItRExANwInFFT5x3AlRGxCSAiHk/LA+gEJgKTgA7gsQLb2lRXRxsdbXJgmFmpFRkYRwDrMst9aVnWMcAxku6QdJekOQARcSfwY+CR9LEoIlbVvoGk8yQtlbS0v7+/kJ1I34dKpycgNLNyKzIw6o05RM1yOzALOAWYB3xR0kGS/hh4EXAkSci8TtKrd9tYxFURMTsiZvf29u7Vxtfq6fL0IGZWbkUGRh8wLbN8JLC+Tp1vR8SOiHgQWE0SIG8C7oqIrRGxFfhP4JUFtnVE3Z6A0MxKrsjAWALMkjRT0kRgLrCgps4twGsBJB1CcohqLfA74DWS2iV1kAx473ZIal/yjLVmVnaFBUZEDALzgUUkX/Y3RcQKSRdLOj2ttgjYIGklyZjFhyJiA3Az8FvgPmA5sDwivlNUW/OodLZ7LikzK7XCTqsFiIiFwMKasgszzwN4f/rI1hkC3llk20bLYxhmVna+0jun6l33kowzMysfB0ZOPV0dDO4Mtg8MtbopZmYt4cDIqTqflK/FMLOycmDkVJ1PyuMYZlZWDoycnr3rnicgNLOScmDk5B6GmZWdAyOnZ8cwHBhmVlIOjJzcwzCzsnNg5NTdmY5h+CwpMyspB0ZO7W0TmDyp3T0MMystB8YoJBMQ+iwpMysnB8YodHe6h2Fm5eXAGIWeLt91z8zKy4ExChXfE8PMSsyBMQq+iZKZlZkDYxQqnb4nhpmVlwNjFHq6Otg2MMTg0M5WN8XMbJ9zYIzCsxMQ+latZlZCDoxRqE4P4nEMMysjB8YoVCcg9DiGmZWRA2MUeg7wXffMrLwcGKPgHoaZlVmhgSFpjqTVktZIuqBBnbMkrZS0QtL1mfLpkr4vaVX6+owi25rH8BiGB73NrHzai9qwpDbgSuBUoA9YImlBRKzM1JkFfAQ4KSI2STo0s4nrgEsiYrGkyUDLz2WtniXlHoaZlVGRPYzjgTURsTYiBoAbgTNq6rwDuDIiNgFExOMAkl4MtEfE4rR8a0RsL7CtuXR1tNHRJo9hmFkpFRkYRwDrMst9aVnWMcAxku6QdJekOZnyzZK+KelXki5Neyy7kHSepKWSlvb39xeyEzXv56u9zay0igwM1SmLmuV2YBZwCjAP+KKkg9Lyk4EPAq8AjgbO3W1jEVdFxOyImN3b27v3Wt6E55Mys7IqMjD6gGmZ5SOB9XXqfDsidkTEg8BqkgDpA36VHs4aBG4BXlZgW3Pr7nIPw8zKqcjAWALMkjRT0kRgLrCgps4twGsBJB1CcihqbbruFEnVbsPrgJWMAck9MXyWlJmVT2GBkfYM5gOLgFXATRGxQtLFkk5Pqy0CNkhaCfwY+FBEbIiIIZLDUT+UdB/J4a0vFNXW0ah0tvuQlJmVUmGn1QJExEJgYU3ZhZnnAbw/fdSuuxg4tsj27QmPYZhZWflK71GqpGMYSdaZmZWHA2OUero6GNwZbB8YanVTzMz2KQfGKFXnk/LFe2ZWNg6MUarOJ+VTa82sbBwYo/TsXfc8AaGZlYwDY5TcwzCzsnJgjNKzYxgODDMrGQfGKLmHYWZl5cAYpe7OdAzDZ0mZWck4MEapvW0Ckye1u4dhZqXjwNgDyXxSPkvKzMrFgbEHKp7i3MxKyIGxBypdHR7DMLPScWDsAc9Ya2Zl5MDYA5VOB4aZlY8DYw/0eAzDzErIgbEHKl3tbBsYYnBoZ6ubYma2zxR6x73xqnq195mfv5NDJk9i6gETmXLgRA4+MPk79cAOph5YLe9g8qR2JLW41WZmz40DYw+8/kWHcc+6zfRveYZ1G7ezfN1mNm0fYMdQ/bvwTWybwJQDOzhgYju7xIbqPt0lXBwzZjaSFx5e4d/mHVf4+zgw9sC0qQfwmbm7fjgRwdZnBtm4bYCN2wbYtH2ADVuTvxu37WDjtmd4esfOXeo/+3yXDWWf+jawZjayaVO69sn7ODD2Ekl0d3bQ3dnBUQcf2OrmmJntdR70NjOzXAoNDElzJK2WtEbSBQ3qnCVppaQVkq6vea0i6feSriiynWZmNrLCDklJagOuBE4F+oAlkhZExMpMnVnAR4CTImKTpENrNvNx4Lai2mhmZvkV2cM4HlgTEWsjYgC4ETijps47gCsjYhNARDxefUHSy4HDgO8X2EYzM8upyMA4AliXWe5Ly7KOAY6RdIekuyTNAZA0AbgM+FCzN5B0nqSlkpb29/fvxaabmVmtIgOj3iUEteeJtgOzgFOAecAXJR0EvAdYGBHraCIiroqI2RExu7e3dy802czMGinytNo+YFpm+UhgfZ06d0XEDuBBSatJAuRE4GRJ7wEmAxMlbY2IugPnZmZWvCJ7GEuAWZJmSpoIzAUW1NS5BXgtgKRDSA5RrY2It0bE9IiYAXwQuM5hYWbWWoX1MCJiUNJ8YBHQBlwdESskXQwsjYgF6WunSVoJDAEfiogNe/J+y5Yte0LSw8+hyYcATzyH9fdn3vfyKvP+l3nfYXj/j8q7grJTVJSZpKURMbvV7WgF73s59x3Kvf9l3nfYs/33ld5mZpaLA8PMzHJxYAy7qtUNaCHve3mVef/LvO+wB/vvMQwzM8vFPQwzM8vFgWFmZrmUPjDyTME+nkl6SNJ9ku6RtLTV7SmSpKslPS7p15myqZIWS3og/TullW0sUoP9vyi9hcA96eMvWtnGokiaJunHklalt1L4H2n5uP/8m+z7qD/7Uo9hpFOw309mCnZgXnYK9vFO0kPA7IgY9xcwSXo1sJVk5oA/Scs+CWyMiE+kPximRMQ/t7KdRWmw/xcBWyPiU61sW9EkHQ4cHhF3S+oGlgF/DZzLOP/8m+z7WYzysy97DyPPFOw2TkTE7cDGmuIzgC+nz79M8j/SuNRg/0shIh6JiLvT51uAVSSzZ4/7z7/Jvo9a2QMjzxTs410A35e0TNJ5rW5MCxwWEY9A8j8WUHsTrzKYL+ne9JDVuDskU0vSDOA44BeU7POv2XcY5Wdf9sDIMwX7eHdSRLwMeAPw3vSwhZXH/wOeD7wUeITkPjTjlqTJwDeAf4qIp1rdnn2pzr6P+rMve2DkmYJ9XIuI9enfx4FvkRymK5PH0mO81WO9j49Qf1yJiMciYigidgJfYBx//pI6SL4wvxoR30yLS/H519v3Pfnsyx4YeaZgH7ckHZgOgiHpQOA04NfN1xp3FgDnpM/PAb7dwrbsc9Uvy9SbGKefvyQBXwJWRcSnMy+N+8+/0b7vyWdf6rOkANJTyf6V4SnYL2lxk/YZSUeT9Cogmer++vG8/5JuILm74yHAY8DHSO7JchMwHfgdcGZEjMuB4Qb7fwrJIYkAHgLeWT2mP55IehXwU+A+YGda/C8kx/LH9effZN/nMcrPvvSBYWZm+ZT9kJSZmeXkwDAzs1wcGGZmlosDw8zMcnFgmJlZLg4MG/Mk/Tz9O0PS3+3lbf9LvfcqiqS/lnRhQdv+l5FrjXqbL5F07d7eru2ffFqt7TcknQJ8MCLeOIp12iJiqMnrWyNi8t5oX872/Bw4/bnODlxvv4raF0k/AP4hIn63t7dt+xf3MGzMk7Q1ffoJ4OR07v7zJbVJulTSknQCtXem9U9J5/+/nuRiJSTdkk6wuKI6yaKkTwBd6fa+mn0vJS6V9Gsl9wt5S2bbP5F0s6TfSPpqeiUtkj4haWXalt2mjJZ0DPBMNSwkXSvp3yX9VNL9kt6Ylufer8y26+3L2yT9Mi37fDqdP5K2SrpE0nJJd0k6LC0/M93f5ZJuz2z+OySzIFjZRYQffozpB8mc/ZBclXxrpvw84KPp80nAUmBmWm8bMDNTd2r6t4tkCoSDs9uu815/CywmmQHgMJKrgA9Pt/0kybxjE4A7gVcBU4HVDPfaD6qzH28HLsssXwt8L93OLJK5zTpHs1/12p4+fxHJF31Huvw54Oz0eQB/lT7/ZOa97gOOqG0/cBLwnVb/d+BH6x/teYPFbAw6DThW0pvT5R6SL94B4JcR8WCm7j9KelP6fFpab0OTbb8KuCGSwz6PSboNeAXwVLrtPgBJ9wAzgLuAPwBflPRd4NY62zwc6K8puymSyd8ekLQWeOEo96uRPwNeDixJO0BdDE+sN5Bp3zKSG4gB3AFcK+km4JvDm+Jx4Hk53tPGOQeG7c8EvC8iFu1SmIx1bKtZfj1wYkRsl/QTkl/yI227kWcyz4eA9ogYlHQ8yRf1XGA+8Lqa9Z4m+fLPqh1EDHLu1wgEfDkiPlLntR0RUX3fIdLvgYh4l6QTgL8E7pH00ojYQPJv9XTO97VxzGMYtj/ZAnRnlhcB706nbkbSMemsu7V6gE1pWLwQeGXmtR3V9WvcDrwlHU/oBV4N/LJRw5Tca6AnIhYC/0QyqVutVcAf15SdKWmCpOcDR5Mc1sq7X7Wy+/JD4M2SDk23MVXSUc1WlvT8iPhFRFwIPMHw1P/HME5nsbXRcQ/D9if3AoOSlpMc//8MyeGgu9OB537q32Lze8C7JN1L8oV8V+a1q4B7Jd0dEW/NlH8LOBFYTvKr/8MR8WgaOPV0A9+W1Eny6/78OnVuBy6TpMwv/NXAbSTjJO+KiD9I+mLO/aq1y75I+ijJ3RQnADu93pV4AAAAeklEQVSA9wIPN1n/Ukmz0vb/MN13gNcC383x/jbO+bRas31I0mdIBpB/kF7fcGtE3NziZjUkaRJJoL0qIgZb3R5rLR+SMtu3/g9wQKsbMQrTgQscFgbuYZiZWU7uYZiZWS4ODDMzy8WBYWZmuTgwzMwsFweGmZnl8v8BxiCfPWk5ijkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims=(12288, 20, 7, 5, 1), learning_rate=0.075, num_iterations=2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
